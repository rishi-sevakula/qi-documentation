"use strict";(globalThis.webpackChunkdocumentation=globalThis.webpackChunkdocumentation||[]).push([[1717],{2354(e,n,t){t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"build/nodes/llm","title":"LLM Node","description":"Call a language model directly with a prompt template.","source":"@site/docs/build/nodes/llm.md","sourceDirName":"build/nodes","slug":"/build/nodes/llm","permalink":"/build/nodes/llm","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"LLM Node","description":"Call a language model directly with a prompt template."},"sidebar":"guideSidebar","previous":{"title":"Start Node","permalink":"/build/nodes/start"},"next":{"title":"Agent Node","permalink":"/build/nodes/agent"}}');var l=t(4848),r=t(8453);const s={title:"LLM Node",description:"Call a language model directly with a prompt template."},o=void 0,d={},a=[{value:"Purpose",id:"purpose",level:2},{value:"When to use",id:"when-to-use",level:2},{value:"Configuration",id:"configuration",level:2},{value:"Model",id:"model",level:3},{value:"Prompt template",id:"prompt-template",level:3},{value:"Structured output (optional)",id:"structured-output-optional",level:3},{value:"Output",id:"output",level:2},{value:"Example",id:"example",level:2},{value:"Things to keep in mind",id:"things-to-keep-in-mind",level:2}];function c(e){const n={code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.p,{children:"The LLM node runs a single model call. Use it for deterministic generation, classification, or structured extraction."}),"\n",(0,l.jsx)(n.h2,{id:"purpose",children:"Purpose"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Generates text from a prompt template."}),"\n",(0,l.jsx)(n.li,{children:"Optionally enforces a structured JSON response schema."}),"\n",(0,l.jsx)(n.li,{children:"Does not call tools (use Agent nodes for tool use)."}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"when-to-use",children:"When to use"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"You want a single-step LLM output."}),"\n",(0,l.jsx)(n.li,{children:"You do not need multi-step reasoning or tool calls."}),"\n",(0,l.jsx)(n.li,{children:"You need predictable output and lower latency than agent loops."}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,l.jsx)(n.h3,{id:"model",children:"Model"}),"\n",(0,l.jsxs)(n.p,{children:["The model object follows ",(0,l.jsx)(n.code,{children:"ModelConfig"})," and supports provider-specific settings:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"provider"}),": ",(0,l.jsx)(n.code,{children:"openai | anthropic | azureopenai | local"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"code"}),": primary model identifier (preferred)"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"name"}),": display name (fallback)"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"version"}),": optional version label"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"credentialId"}),": references stored credentials"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"temperature"}),", ",(0,l.jsx)(n.code,{children:"maxTokens"}),", ",(0,l.jsx)(n.code,{children:"topP"}),", ",(0,l.jsx)(n.code,{children:"frequencyPenalty"}),", ",(0,l.jsx)(n.code,{children:"presencePenalty"})]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"prompt-template",children:"Prompt template"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.code,{children:"promptTemplate"})," is required and must contain at least one message:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-json",children:'{\n  "id": "uuid",\n  "role": "system | user | assistant | tool",\n  "text": "string with {{variable}} references"\n}\n'})}),"\n",(0,l.jsx)(n.h3,{id:"structured-output-optional",children:"Structured output (optional)"}),"\n",(0,l.jsxs)(n.p,{children:["Use ",(0,l.jsx)(n.code,{children:"structuredOutput.enable = true"})," with a JSON schema to enforce output shape."]}),"\n",(0,l.jsx)(n.h2,{id:"output",children:"Output"}),"\n",(0,l.jsx)(n.p,{children:"The LLM node returns a fixed payload:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"text"}),": the generated text"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"structuredOutput"}),": present when structured output is enabled"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"nodeId"}),", ",(0,l.jsx)(n.code,{children:"nodeType"})]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-json",children:'{\n  "type": "llm",\n  "config": {\n    "model": {\n      "provider": "openai",\n      "code": "gpt-4.1",\n      "temperature": 0.2,\n      "maxTokens": 1024\n    },\n    "promptTemplate": [\n      { "id": "sys-1", "role": "system", "text": "You are a concise assistant." },\n      { "id": "usr-1", "role": "user", "text": "Summarize {{flow.documentText}}." }\n    ],\n    "structuredOutput": {\n      "enable": true,\n      "name": "Summary",\n      "description": "Document summary and key points",\n      "schema": {\n        "type": "object",\n        "properties": {\n          "summary": { "type": "string" },\n          "keyPoints": { "type": "array", "items": { "type": "string" } }\n        },\n        "required": ["summary"]\n      }\n    }\n  }\n}\n'})}),"\n",(0,l.jsx)(n.h2,{id:"things-to-keep-in-mind",children:"Things to keep in mind"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"promptTemplate"})," is required; an empty template fails validation."]}),"\n",(0,l.jsx)(n.li,{children:"Avoid high temperatures when you need deterministic structured output."}),"\n",(0,l.jsx)(n.li,{children:"Streaming can be enabled in config, but downstream nodes still receive the final payload."}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(c,{...e})}):c(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>o});var i=t(6540);const l={},r=i.createContext(l);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);